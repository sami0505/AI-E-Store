{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# # Read data into a dataframe and \n",
    "# df = pd.read_csv('ratings.csv', dtype={\"vote\": str})\n",
    "# counts = df.groupby('reviewerID')['reviewerID'].size()\n",
    "# counts = counts[(counts >=5) & (counts <= 10)]\n",
    "# densityPlot = sns.kdeplot(counts, bw_adjust=3)\n",
    "# yLabels = densityPlot.get_yticks()\n",
    "# densityPlot.set_yticklabels('{:,.0%}'.format(y) for y in yLabels)\n",
    "# plt.xlabel(\"Number of reviews\")\n",
    "# plt.ylabel(\"Percentage of users\")\n",
    "# plt.savefig(fname=\"densityPlot\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Read ratings into a dataframe and narrow down reviewers to desired review count\n",
    "# ratings = pd.read_csv('ratings.csv', dtype={\"vote\": str})\n",
    "# counts = ratings.groupby('reviewerID')['reviewerID'].size()\n",
    "# counts = counts[(counts >=5) & (counts <= 10)]\n",
    "\n",
    "# # Create and apply mask that removes all other reviewers.\n",
    "# mask = ratings['reviewerID'].isin(counts.index)\n",
    "# ratings = ratings[mask]\n",
    "\n",
    "# # Read metadata into dataframe and define asins that are used by making a set\n",
    "# metadata = pd.read_csv(\"metadata.csv\")\n",
    "# used_asins = set(ratings[\"asin\"])\n",
    "\n",
    "# # Create and apply mask that removes unreferenced items\n",
    "# mask = metadata[\"asin\"].isin(used_asins)\n",
    "# metadata = metadata[mask]\n",
    "# metadata.to_pickle(\"metadata.pickle\")\n",
    "# ratings.to_pickle(\"ratings.pickle\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from scipy.sparse import csr_matrix, save_npz\n",
    "# ratings : pd.DataFrame = pd.read_pickle(\"ratings.pickle\")\n",
    "# metadata : pd.DataFrame = pd.read_pickle(\"metadata.pickle\")\n",
    "# ratings = ratings[[\"reviewerID\", \"asin\", \"overall\"]]\n",
    "\n",
    "# # Series of each unique value of reviewerID and asin \n",
    "# reviewerIDs = ratings['reviewerID'].unique()\n",
    "# asins = ratings[\"asin\"].unique()\n",
    "# # Dictionary that maps reviewers to an index\n",
    "# reviewerMap = {reviewerID: i for i, reviewerID in enumerate(reviewerIDs)}\n",
    "\n",
    "# # Dictionary that maps asins to an index\n",
    "# asinMap = {asin: i for i, asin in enumerate(asins)}\n",
    "\n",
    "# # Initialize the utility matrix with zeros\n",
    "# utilityMatrixArray = np.zeros((len(reviewerIDs), len(asins)), \"int8\")\n",
    "\n",
    "# # Fill in the utility matrix with data from the records array\n",
    "# for row in ratings.itertuples():\n",
    "#     reviewerIndex = reviewerMap[row.reviewerID]\n",
    "#     asinIndex = asinMap[row.asin]\n",
    "#     utilityMatrixArray[reviewerIndex, asinIndex] = row.overall\n",
    "# utilityMatrix = csr_matrix(utilityMatrixArray)\n",
    "# save_npz(\"utilityMatrix.npz\", utilityMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "fillna not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m anonymous_user \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame({\u001b[39m'\u001b[39m\u001b[39mitem_id\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m4\u001b[39m], \u001b[39m'\u001b[39m\u001b[39mrating\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m4\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m5\u001b[39m, \u001b[39m2\u001b[39m]})\n\u001b[0;32m     18\u001b[0m \u001b[39m# Compute the similarity between the anonymous user and every other user\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m user_similarity_scores \u001b[39m=\u001b[39m cosine_similarity(utilityMatrix\u001b[39m.\u001b[39;49mfillna(\u001b[39m0\u001b[39m), anonymous_user\u001b[39m.\u001b[39mset_index(\u001b[39m'\u001b[39m\u001b[39mitem_id\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mT\u001b[39m.\u001b[39mfillna(\u001b[39m0\u001b[39m))\n\u001b[0;32m     21\u001b[0m \u001b[39m# Get the k most similar users to the anonymous user\u001b[39;00m\n\u001b[0;32m     22\u001b[0m k \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Sami\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\sparse\\_base.py:764\u001b[0m, in \u001b[0;36mspmatrix.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    762\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgetnnz()\n\u001b[0;32m    763\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 764\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(attr \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m not found\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: fillna not found"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import load_npz, csr_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "ratings : pd.DataFrame = pd.read_pickle(\"ratings.pickle\")\n",
    "metadata : pd.DataFrame = pd.read_pickle(\"metadata.pickle\")\n",
    "utilityMatrix = load_npz(\"utilityMatrix.npz\")\n",
    "\n",
    "# Compute the pairwise cosine similarity between users\n",
    "user_similarity = cosine_similarity(utilityMatrix, dense_output=False)\n",
    "\n",
    "# Create an anonymous user with four reviews\n",
    "anonymous_user = pd.DataFrame({'item_id': [1, 2, 3, 4], 'rating': [4, 3, 5, 2]})\n",
    "\n",
    "# Compute the similarity between the anonymous user and every other user\n",
    "user_similarity_scores = cosine_similarity(utilityMatrix, anonymous_user.set_index('item_id').T.fillna(0))\n",
    "\n",
    "# Get the k most similar users to the anonymous user\n",
    "k = 10\n",
    "similar_user_indices = np.argsort(user_similarity_scores)[0][-k:]\n",
    "\n",
    "# Compute the predicted ratings for the anonymous user\n",
    "predicted_ratings = utilityMatrix.iloc[similar_user_indices].mean().sort_values(ascending=False)\n",
    "\n",
    "# Recommend the top item\n",
    "top_item = predicted_ratings.index[0]\n",
    "\n",
    "# ratings = ratings.rename(columns={\"reviewerID\": \"user_id\", \"asin\" : \"item_id\", \"overall\": \"rating\"})\n",
    "# # Define a Reader object to parse the dataframe into the Surprise dataset format\n",
    "# reader = Reader(rating_scale=(1, 5))\n",
    "# data = Dataset.load_from_df(ratings[['user_id', 'item_id', 'rating']], reader)\n",
    "\n",
    "# # Split the data into training and testing datasets\n",
    "# train_data, test_data = train_test_split(data, test_size=0.2)\n",
    "\n",
    "# # Use KNNWithMeans algorithm to fit the training dataset\n",
    "# sim_options = {'name': 'cosine', 'user_based': True} # similarity metric and user-based filtering\n",
    "# algo = KNNWithMeans(k=50, sim_options=sim_options)\n",
    "# algo.fit(train_data)\n",
    "\n",
    "# # Evaluate the model's accuracy on the testing data\n",
    "# predictions = algo.test(test_data)\n",
    "# accuracy.rmse(predictions)\n",
    "\n",
    "# # Predict ratings for the anonymous user\n",
    "# user_ratings = pd.DataFrame({'user_id': [0, 0, 0, 0], 'item_id': [1, 2, 3, 4], 'rating': [4, 3, 5, 2]})\n",
    "# user_data = Dataset.load_from_df(user_ratings, reader)\n",
    "# user_predictions = algo.test(user_data.build_full_trainset().build_anti_testset())\n",
    "\n",
    "# # Recommend the top item\n",
    "# top_item = max(user_predictions, key=lambda x: x.est).iid\n",
    "\n",
    "\n",
    "\n",
    "# # Create training and testing datasets\n",
    "# trainingData = utilityMatrix[:113404]\n",
    "# testingData = utilityMatrix[113404:]\n",
    "\n",
    "# # X_train, X_test= train_test_split(utilityMatrix, test_size=0.2, random_state=42)\n",
    "# model = TruncatedSVD(n_components=50, random_state=42).fit(trainingData)\n",
    "# resultant_matrix = model.transform(trainingData)\n",
    "# var_explained = model.explained_variance_ratio_.sum()\n",
    "# var_explained"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "79b64650700e93176e44f3302dc5d66caf1d57b10c2f77dc934e38ab1cdc6599"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
